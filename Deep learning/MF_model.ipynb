{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwcSDUbSjcln"
      },
      "source": [
        "## Reader의 추천 알고리즘\n",
        "이 노트북은 Reader의 추천알고리즘을 구현하기 위해 Kaggle의 도서 평점 데이터를 전처리, 학습시키는 코드를 담고 있습니다. 데이터는 아래 url에서 받으실 수 있습니다.\n",
        "\n",
        "▶ [도서 평점 데이터](https://www.kaggle.com/bahramjannesarr/goodreads-book-datasets-10m?select=user_rating_0_to_1000.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpKXtISNidUo",
        "outputId": "7c761fad-a11f-4a76-8715-1c7df446774a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "u_cols = ['user_id', 'book', 'rating']\n",
        "ratings = pd.read_csv('/content/user_rating_0_to_1000.csv', sep=',', names=u_cols, encoding='latin-1')\n",
        "\n",
        "ratings.set_index('user_id')\n",
        "print(ratings)\n",
        "\n",
        "drop_idx = ratings[ratings['book'] == 'Rating'].index \n",
        "ratings = ratings.drop(drop_idx) # 평가데이터가 존재하지 않는 사용자를 제거\n",
        "print(ratings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      user_id  ...           rating\n",
            "0          ID  ...           Rating\n",
            "1           1  ...   it was amazing\n",
            "2           1  ...   it was amazing\n",
            "3           1  ...   it was amazing\n",
            "4           1  ...  really liked it\n",
            "...       ...  ...              ...\n",
            "51941     999  ...        it was ok\n",
            "51942     999  ...   it was amazing\n",
            "51943     999  ...   it was amazing\n",
            "51944     999  ...   it was amazing\n",
            "51945     999  ...   it was amazing\n",
            "\n",
            "[51946 rows x 3 columns]\n",
            "      user_id  ...           rating\n",
            "0          ID  ...           Rating\n",
            "1           1  ...   it was amazing\n",
            "2           1  ...   it was amazing\n",
            "3           1  ...   it was amazing\n",
            "4           1  ...  really liked it\n",
            "...       ...  ...              ...\n",
            "51941     999  ...        it was ok\n",
            "51942     999  ...   it was amazing\n",
            "51943     999  ...   it was amazing\n",
            "51944     999  ...   it was amazing\n",
            "51945     999  ...   it was amazing\n",
            "\n",
            "[51643 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMZs9TmNU5m8"
      },
      "source": [
        "### book 데이터 전처리\n",
        "책 데이터를 처리하기 쉽게, 각 책마다 아이디를 부여합니다.\n",
        "또한, 몇 권의 책이 데이터에 포함되어 있는지, 평가수가 가장 많은 책은 어떤 책인지 확인해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqu0Y-rp2p2J",
        "outputId": "fcc074ca-70c1-4eac-d41c-5957282ae5b1"
      },
      "source": [
        "books = np.array(ratings['book'])\n",
        "\n",
        "id_to_book = {}\n",
        "book_to_id = {}\n",
        "num_book = {}\n",
        "\n",
        "for book in books:\n",
        "  if book not in id_to_book.values():\n",
        "    new_idx = len(id_to_book)\n",
        "    id_to_book[new_idx] = book\n",
        "    book_to_id[book] = new_idx\n",
        "    num_book[new_idx] = 1\n",
        "  else:\n",
        "    num_book[book_to_id[book]] += 1\n",
        "\n",
        "print(\"책 종류 수 : %d\" % len(id_to_book))\n",
        "\n",
        "num_book_sort = sorted(num_book.items(), reverse=True, key = lambda x : x[1])\n",
        "key_max = num_book_sort[0][0]\n",
        "print(\"가장 평가데이터가 많은 책 : %s %d\" % (id_to_book[key_max], num_book[key_max]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "책 종류 수 : 24094\n",
            "가장 평가데이터가 많은 책 : The Kite Runner 126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdBWzG1WU5a"
      },
      "source": [
        "평가데이터 도서 항목을 제목 대신 아이디로 교체합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwsZgDWl4JAe",
        "outputId": "cb4e041c-f946-44ca-ae08-1e0a2e0e0e69"
      },
      "source": [
        "ratings['book'] = ratings['book'].apply(lambda x : book_to_id[x])\n",
        "print(ratings['book'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0            0\n",
            "1            1\n",
            "2            2\n",
            "3            3\n",
            "4            4\n",
            "         ...  \n",
            "51940    24092\n",
            "51941    15354\n",
            "51942      448\n",
            "51943    12404\n",
            "51944     7602\n",
            "Name: book, Length: 51642, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P00Kf8yVWfyH"
      },
      "source": [
        "### 점수 데이터 전처리\n",
        "문장으로 되어 있는 평가데이터를 숫자로 변환합니다. 사용자들이 가장 많이 매긴 점수 항목을 확인하고, 데이터 형식을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0WI_KY74FxV",
        "outputId": "1164d47c-130f-4656-b675-8f48a6baa025"
      },
      "source": [
        "ratings_val = np.array(ratings['rating'])\n",
        "\n",
        "id_to_rating = {}\n",
        "rating_to_id = {}\n",
        "num = {}\n",
        "\n",
        "for rating in ratings_val:\n",
        "  if rating not in rating_to_id.keys():\n",
        "    new_idx = len(rating_to_id)\n",
        "    id_to_rating[new_idx] = rating\n",
        "    rating_to_id[rating] = new_idx\n",
        "    num[new_idx] = 1\n",
        "  else:\n",
        "    num[rating_to_id[rating]] += 1\n",
        "\n",
        "\"\"\"\n",
        "it was amazing - 5점\n",
        "really liked it - 4점\n",
        "liked it - 3점\n",
        "it was ok - 2점\n",
        "did not like it - 1점\n",
        "\"\"\"\n",
        "\n",
        "num_sort = sorted(num.items(), reverse=True, key = lambda x : x[1])\n",
        "key_max = num_sort[0][0]\n",
        "print(id_to_rating[key_max], num[key_max])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "really liked it 20282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fVHp30rXFLX"
      },
      "source": [
        "점수 데이터를 모두 숫자로 변경합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6sZ8mDV86_b",
        "outputId": "7eb5dcf6-6bdb-44b1-8243-e1b33f4e0b10"
      },
      "source": [
        "rating_to_id['did not like it'] = 1\n",
        "rating_to_id['it was ok'] = 2\n",
        "rating_to_id['liked it'] = 3\n",
        "rating_to_id['really liked it'] = 4\n",
        "rating_to_id['it was amazing'] = 5\n",
        "\n",
        "ratings['rating'] = ratings['rating'].apply(lambda x : rating_to_id[x])\n",
        "print(ratings['rating'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        5\n",
            "1        5\n",
            "2        5\n",
            "3        4\n",
            "4        4\n",
            "        ..\n",
            "51940    2\n",
            "51941    5\n",
            "51942    5\n",
            "51943    5\n",
            "51944    5\n",
            "Name: rating, Length: 51642, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U28FdLXFXNY-"
      },
      "source": [
        "1부터 시작하는 사용자 아이디를 1씩 줄여서 0부터 시작되도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnBztmbgWy8f",
        "outputId": "fdf4e731-9f0b-446b-cf2d-6db74d989431"
      },
      "source": [
        "ratings['user_id'] = ratings['user_id'].apply(lambda x : x-1)\n",
        "print(ratings['user_id'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0          0\n",
            "1          0\n",
            "2          0\n",
            "3          0\n",
            "4          0\n",
            "        ... \n",
            "51940    998\n",
            "51941    998\n",
            "51942    998\n",
            "51943    998\n",
            "51944    998\n",
            "Name: user_id, Length: 51642, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-sn7ddbXZz8"
      },
      "source": [
        "### 사용자 평점을 예측하는 딥러닝 모델 구축\n",
        "MF(행렬 분해)를 기반으로 한 model-based 딥러닝 모델로, 사용자 평점 예측 모델을 구축합니다. 모델 학습 이후, 가중치를 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "laXxyCAg9GWI",
        "outputId": "fe5e1dc2-b080-436f-8185-848f10a71a95"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Activation, Flatten, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adamax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "TRAIN_SIZE = 0.75\n",
        "ratings = shuffle(ratings, random_state = 1)\n",
        "cut_off = int(len(ratings) * 0.75)\n",
        "ratings_train = ratings[:cut_off]\n",
        "ratings_test = ratings[cut_off:]\n",
        "\n",
        "K = 200\n",
        "mu = ratings_train['rating'].mean()\n",
        "M = ratings['user_id'].max() + 1\n",
        "N = ratings['book'].max() + 1\n",
        "\n",
        "# 손실함수\n",
        "def RMSE(y_true, y_pred):\n",
        "  return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
        "\n",
        "user = Input(shape = (1,))\n",
        "item = Input(shape = (1,))\n",
        "P_embedding = Embedding(M, K, embeddings_regularizer=l2(), name='user_vec')(user)\n",
        "Q_embedding = Embedding(N, K, embeddings_regularizer=l2(), name='book_vec')(item)\n",
        "user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)\n",
        "item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)\n",
        "\n",
        "P_embedding = Flatten()(P_embedding)\n",
        "Q_embedding = Flatten()(Q_embedding)\n",
        "user_bias = Flatten()(user_bias)\n",
        "item_bias = Flatten()(item_bias)\n",
        "\n",
        "R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])\n",
        "R = Dense(2048)(R)\n",
        "R = Activation('relu')(R)\n",
        "R = Dropout(0.3)(R)\n",
        "R = Dense(256)(R)\n",
        "R = Activation('relu')(R)\n",
        "R = Dense(1)(R)\n",
        "\n",
        "model = Model(inputs = [user, item], outputs = R)\n",
        "model.compile(\n",
        "    loss = RMSE,\n",
        "    optimizer = SGD(),\n",
        "    metrics = [RMSE]\n",
        ")\n",
        "\n",
        "results = model.fit(\n",
        "    x = [ratings_train['user_id'].values, ratings_train['book'].values],\n",
        "    y = ratings_train['rating'].values - mu,\n",
        "    epochs = 100,\n",
        "    batch_size = 512,\n",
        "    validation_data = (\n",
        "        [ratings_test['user_id'].values, ratings_test['book'].values],\n",
        "        ratings_test['rating'].values - mu\n",
        "    )\n",
        ")\n",
        "\n",
        "# 모델 저장하기\n",
        "model.save('book_recom_model.h5')\n",
        "model.save_weights('model_weight.h5')\n",
        "\n",
        "# 결과값 시각화 하기\n",
        "plt.plot(results.history['RMSE'], label = \"Train RMSE\")\n",
        "plt.plot(results.history['val_RMSE'], label = \"Test RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "76/76 [==============================] - 9s 111ms/step - loss: 42.6892 - RMSE: 0.9425 - val_loss: 41.7489 - val_RMSE: 0.9439\n",
            "Epoch 2/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 41.4424 - RMSE: 0.9458 - val_loss: 40.5266 - val_RMSE: 0.9435\n",
            "Epoch 3/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 40.2318 - RMSE: 0.9479 - val_loss: 39.3410 - val_RMSE: 0.9432\n",
            "Epoch 4/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 39.0530 - RMSE: 0.9455 - val_loss: 38.1910 - val_RMSE: 0.9430\n",
            "Epoch 5/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 37.9165 - RMSE: 0.9501 - val_loss: 37.0753 - val_RMSE: 0.9427\n",
            "Epoch 6/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 36.8051 - RMSE: 0.9456 - val_loss: 35.9930 - val_RMSE: 0.9425\n",
            "Epoch 7/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 35.7301 - RMSE: 0.9445 - val_loss: 34.9432 - val_RMSE: 0.9422\n",
            "Epoch 8/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 34.6868 - RMSE: 0.9429 - val_loss: 33.9248 - val_RMSE: 0.9419\n",
            "Epoch 9/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 33.6754 - RMSE: 0.9419 - val_loss: 32.9368 - val_RMSE: 0.9416\n",
            "Epoch 10/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 32.6951 - RMSE: 0.9418 - val_loss: 31.9785 - val_RMSE: 0.9413\n",
            "Epoch 11/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 31.7489 - RMSE: 0.9464 - val_loss: 31.0487 - val_RMSE: 0.9410\n",
            "Epoch 12/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 30.8201 - RMSE: 0.9401 - val_loss: 30.1468 - val_RMSE: 0.9406\n",
            "Epoch 13/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 29.9235 - RMSE: 0.9382 - val_loss: 29.2720 - val_RMSE: 0.9404\n",
            "Epoch 14/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 29.0604 - RMSE: 0.9430 - val_loss: 28.4233 - val_RMSE: 0.9400\n",
            "Epoch 15/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 28.2134 - RMSE: 0.9380 - val_loss: 27.5999 - val_RMSE: 0.9396\n",
            "Epoch 16/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 27.3960 - RMSE: 0.9373 - val_loss: 26.8012 - val_RMSE: 0.9392\n",
            "Epoch 17/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 26.6065 - RMSE: 0.9401 - val_loss: 26.0264 - val_RMSE: 0.9388\n",
            "Epoch 18/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 25.8367 - RMSE: 0.9389 - val_loss: 25.2748 - val_RMSE: 0.9384\n",
            "Epoch 19/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 25.0889 - RMSE: 0.9366 - val_loss: 24.5457 - val_RMSE: 0.9380\n",
            "Epoch 20/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 24.3666 - RMSE: 0.9375 - val_loss: 23.8383 - val_RMSE: 0.9375\n",
            "Epoch 21/100\n",
            "76/76 [==============================] - 8s 111ms/step - loss: 23.6682 - RMSE: 0.9407 - val_loss: 23.1520 - val_RMSE: 0.9369\n",
            "Epoch 22/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 22.9831 - RMSE: 0.9361 - val_loss: 22.4864 - val_RMSE: 0.9364\n",
            "Epoch 23/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 22.3270 - RMSE: 0.9401 - val_loss: 21.8405 - val_RMSE: 0.9358\n",
            "Epoch 24/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 21.6866 - RMSE: 0.9401 - val_loss: 21.2140 - val_RMSE: 0.9353\n",
            "Epoch 25/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 21.0639 - RMSE: 0.9386 - val_loss: 20.6062 - val_RMSE: 0.9346\n",
            "Epoch 26/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 20.4532 - RMSE: 0.9305 - val_loss: 20.0166 - val_RMSE: 0.9340\n",
            "Epoch 27/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 19.8739 - RMSE: 0.9357 - val_loss: 19.4444 - val_RMSE: 0.9331\n",
            "Epoch 28/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 19.3037 - RMSE: 0.9325 - val_loss: 18.8896 - val_RMSE: 0.9325\n",
            "Epoch 29/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 18.7536 - RMSE: 0.9325 - val_loss: 18.3511 - val_RMSE: 0.9316\n",
            "Epoch 30/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 18.2170 - RMSE: 0.9294 - val_loss: 17.8286 - val_RMSE: 0.9307\n",
            "Epoch 31/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 17.7040 - RMSE: 0.9340 - val_loss: 17.3218 - val_RMSE: 0.9297\n",
            "Epoch 32/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 17.1982 - RMSE: 0.9303 - val_loss: 16.8302 - val_RMSE: 0.9288\n",
            "Epoch 33/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 16.7140 - RMSE: 0.9331 - val_loss: 16.3531 - val_RMSE: 0.9278\n",
            "Epoch 34/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 16.2387 - RMSE: 0.9302 - val_loss: 15.8901 - val_RMSE: 0.9265\n",
            "Epoch 35/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 15.7702 - RMSE: 0.9199 - val_loss: 15.4411 - val_RMSE: 0.9255\n",
            "Epoch 36/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 15.3291 - RMSE: 0.9234 - val_loss: 15.0054 - val_RMSE: 0.9243\n",
            "Epoch 37/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 14.9008 - RMSE: 0.9263 - val_loss: 14.5826 - val_RMSE: 0.9230\n",
            "Epoch 38/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 14.4749 - RMSE: 0.9187 - val_loss: 14.1725 - val_RMSE: 0.9217\n",
            "Epoch 39/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 14.0670 - RMSE: 0.9166 - val_loss: 13.7747 - val_RMSE: 0.9205\n",
            "Epoch 40/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 13.6780 - RMSE: 0.9211 - val_loss: 13.3884 - val_RMSE: 0.9189\n",
            "Epoch 41/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 13.2926 - RMSE: 0.9175 - val_loss: 13.0138 - val_RMSE: 0.9175\n",
            "Epoch 42/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 12.9247 - RMSE: 0.9200 - val_loss: 12.6505 - val_RMSE: 0.9162\n",
            "Epoch 43/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 12.5655 - RMSE: 0.9200 - val_loss: 12.2978 - val_RMSE: 0.9146\n",
            "Epoch 44/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 12.2128 - RMSE: 0.9158 - val_loss: 11.9558 - val_RMSE: 0.9132\n",
            "Epoch 45/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 11.8731 - RMSE: 0.9142 - val_loss: 11.6239 - val_RMSE: 0.9117\n",
            "Epoch 46/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 11.5405 - RMSE: 0.9095 - val_loss: 11.3019 - val_RMSE: 0.9102\n",
            "Epoch 47/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 11.2222 - RMSE: 0.9093 - val_loss: 10.9896 - val_RMSE: 0.9089\n",
            "Epoch 48/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 10.9119 - RMSE: 0.9075 - val_loss: 10.6867 - val_RMSE: 0.9075\n",
            "Epoch 49/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 10.6155 - RMSE: 0.9104 - val_loss: 10.3926 - val_RMSE: 0.9060\n",
            "Epoch 50/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 10.3155 - RMSE: 0.9007 - val_loss: 10.1075 - val_RMSE: 0.9047\n",
            "Epoch 51/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 10.0345 - RMSE: 0.9014 - val_loss: 9.8309 - val_RMSE: 0.9033\n",
            "Epoch 52/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 9.7625 - RMSE: 0.9026 - val_loss: 9.5626 - val_RMSE: 0.9021\n",
            "Epoch 53/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 9.4913 - RMSE: 0.8964 - val_loss: 9.3024 - val_RMSE: 0.9010\n",
            "Epoch 54/100\n",
            "76/76 [==============================] - 8s 105ms/step - loss: 9.2417 - RMSE: 0.9039 - val_loss: 9.0499 - val_RMSE: 0.8997\n",
            "Epoch 55/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 8.9871 - RMSE: 0.8987 - val_loss: 8.8052 - val_RMSE: 0.8988\n",
            "Epoch 56/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 8.7509 - RMSE: 0.9044 - val_loss: 8.5676 - val_RMSE: 0.8977\n",
            "Epoch 57/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 8.5087 - RMSE: 0.8969 - val_loss: 8.3373 - val_RMSE: 0.8968\n",
            "Epoch 58/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 8.2850 - RMSE: 0.9008 - val_loss: 8.1137 - val_RMSE: 0.8957\n",
            "Epoch 59/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 8.0620 - RMSE: 0.8987 - val_loss: 7.8972 - val_RMSE: 0.8950\n",
            "Epoch 60/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 7.8440 - RMSE: 0.8949 - val_loss: 7.6868 - val_RMSE: 0.8940\n",
            "Epoch 61/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 7.6299 - RMSE: 0.8887 - val_loss: 7.4834 - val_RMSE: 0.8938\n",
            "Epoch 62/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 7.4290 - RMSE: 0.8893 - val_loss: 7.2851 - val_RMSE: 0.8925\n",
            "Epoch 63/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 7.2375 - RMSE: 0.8934 - val_loss: 7.0933 - val_RMSE: 0.8918\n",
            "Epoch 64/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 7.0491 - RMSE: 0.8947 - val_loss: 6.9071 - val_RMSE: 0.8911\n",
            "Epoch 65/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 6.8563 - RMSE: 0.8859 - val_loss: 6.7264 - val_RMSE: 0.8902\n",
            "Epoch 66/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 6.6760 - RMSE: 0.8842 - val_loss: 6.5513 - val_RMSE: 0.8896\n",
            "Epoch 67/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 6.5055 - RMSE: 0.8868 - val_loss: 6.3815 - val_RMSE: 0.8891\n",
            "Epoch 68/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 6.3367 - RMSE: 0.8861 - val_loss: 6.2171 - val_RMSE: 0.8889\n",
            "Epoch 69/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 6.1752 - RMSE: 0.8875 - val_loss: 6.0567 - val_RMSE: 0.8879\n",
            "Epoch 70/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 6.0125 - RMSE: 0.8829 - val_loss: 5.9017 - val_RMSE: 0.8874\n",
            "Epoch 71/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 5.8616 - RMSE: 0.8853 - val_loss: 5.7519 - val_RMSE: 0.8874\n",
            "Epoch 72/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 5.7128 - RMSE: 0.8852 - val_loss: 5.6054 - val_RMSE: 0.8863\n",
            "Epoch 73/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 5.5671 - RMSE: 0.8838 - val_loss: 5.4638 - val_RMSE: 0.8858\n",
            "Epoch 74/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 5.4257 - RMSE: 0.8825 - val_loss: 5.3265 - val_RMSE: 0.8853\n",
            "Epoch 75/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 5.2892 - RMSE: 0.8817 - val_loss: 5.1940 - val_RMSE: 0.8855\n",
            "Epoch 76/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 5.1560 - RMSE: 0.8803 - val_loss: 5.0643 - val_RMSE: 0.8846\n",
            "Epoch 77/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 5.0274 - RMSE: 0.8794 - val_loss: 4.9390 - val_RMSE: 0.8842\n",
            "Epoch 78/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 4.9036 - RMSE: 0.8796 - val_loss: 4.8175 - val_RMSE: 0.8839\n",
            "Epoch 79/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 4.7832 - RMSE: 0.8794 - val_loss: 4.6993 - val_RMSE: 0.8833\n",
            "Epoch 80/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 4.6730 - RMSE: 0.8858 - val_loss: 4.5850 - val_RMSE: 0.8830\n",
            "Epoch 81/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 4.5518 - RMSE: 0.8778 - val_loss: 4.4740 - val_RMSE: 0.8826\n",
            "Epoch 82/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 4.4458 - RMSE: 0.8816 - val_loss: 4.3664 - val_RMSE: 0.8822\n",
            "Epoch 83/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 4.3350 - RMSE: 0.8773 - val_loss: 4.2620 - val_RMSE: 0.8819\n",
            "Epoch 84/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 4.2371 - RMSE: 0.8826 - val_loss: 4.1607 - val_RMSE: 0.8816\n",
            "Epoch 85/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 4.1359 - RMSE: 0.8816 - val_loss: 4.0624 - val_RMSE: 0.8812\n",
            "Epoch 86/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 4.0347 - RMSE: 0.8776 - val_loss: 3.9671 - val_RMSE: 0.8808\n",
            "Epoch 87/100\n",
            "76/76 [==============================] - 8s 110ms/step - loss: 3.9421 - RMSE: 0.8792 - val_loss: 3.8747 - val_RMSE: 0.8806\n",
            "Epoch 88/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 3.8477 - RMSE: 0.8763 - val_loss: 3.7849 - val_RMSE: 0.8802\n",
            "Epoch 89/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 3.7537 - RMSE: 0.8710 - val_loss: 3.6981 - val_RMSE: 0.8802\n",
            "Epoch 90/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 3.6715 - RMSE: 0.8749 - val_loss: 3.6142 - val_RMSE: 0.8803\n",
            "Epoch 91/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 3.5906 - RMSE: 0.8774 - val_loss: 3.5316 - val_RMSE: 0.8794\n",
            "Epoch 92/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 3.5019 - RMSE: 0.8698 - val_loss: 3.4529 - val_RMSE: 0.8797\n",
            "Epoch 93/100\n",
            "76/76 [==============================] - 8s 110ms/step - loss: 3.4318 - RMSE: 0.8782 - val_loss: 3.3750 - val_RMSE: 0.8787\n",
            "Epoch 94/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 3.3523 - RMSE: 0.8748 - val_loss: 3.3003 - val_RMSE: 0.8784\n",
            "Epoch 95/100\n",
            "76/76 [==============================] - 8s 109ms/step - loss: 3.2790 - RMSE: 0.8754 - val_loss: 3.2277 - val_RMSE: 0.8781\n",
            "Epoch 96/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 3.2063 - RMSE: 0.8744 - val_loss: 3.1576 - val_RMSE: 0.8781\n",
            "Epoch 97/100\n",
            "76/76 [==============================] - 8s 108ms/step - loss: 3.1344 - RMSE: 0.8721 - val_loss: 3.0895 - val_RMSE: 0.8779\n",
            "Epoch 98/100\n",
            "76/76 [==============================] - 8s 106ms/step - loss: 3.0672 - RMSE: 0.8723 - val_loss: 3.0230 - val_RMSE: 0.8773\n",
            "Epoch 99/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 3.0034 - RMSE: 0.8740 - val_loss: 2.9587 - val_RMSE: 0.8770\n",
            "Epoch 100/100\n",
            "76/76 [==============================] - 8s 107ms/step - loss: 2.9362 - RMSE: 0.8702 - val_loss: 2.8965 - val_RMSE: 0.8768\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1juYx56aLqDN",
        "outputId": "9a3826fc-5920-4969-8882-bc5d2a3fa059"
      },
      "source": [
        "model = Model(inputs = [user, item], outputs = R)\n",
        "model.load_weights(filepath='/content/model_weight.h5')\n",
        "\n",
        "books = pd.read_csv('/content/book_1000.csv', sep=',', index_col='id', encoding='latin-1')\n",
        "\n",
        "for i in range(100):\n",
        "  book_ids = np.array(books.index)\n",
        "  user_ids = np.array([i] * 1000)\n",
        "  predictions = model.predict([user_ids, book_ids]) + mu\n",
        "  predictions = predictions.reshape(-1)\n",
        "  idx = np.argsort(predictions)[::-1]\n",
        "  print(book_ids[idx][:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158 1196  270 3686  737  569\n",
            "   98  448  728 2800  564  272  370  261  207  500  875  414  376  833\n",
            "  349 2843  195 1129 2229 1009 1143  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61 1612  359  503  786  158 1196  270 3686   98  569\n",
            "  737  728  564  370  448  500 2800  875  272  207  414  261  349  376\n",
            " 2229  833 1143  195  706 2843 3926 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158  270 1196 3686  569   98\n",
            "  564  448  737  728  370  875  272  500 2800  207  261  414 2229  195\n",
            "  376  349 2843  833 1009 1143 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  786  503  158 1196  270 3686  569   98\n",
            "  737 2800  448  728  261  564  207  500  272  370  875  376  414  833\n",
            "  349  195 2843 2229 1143 1129 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  737\n",
            "  564  448   98  728  370 2800  875  207  500  272  261  349  376  195\n",
            "  414  833 2843 1143 1009 2229  706 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158 1196  270 3686  569  737\n",
            "  564  448   98  728  207 2800  370  261  875  500  272  833  376  349\n",
            "  414 2843  195 1009 1143 2229 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  231 1381\n",
            "  471  355   27   61  503  359  786 1612  158 1196  270 3686   98  569\n",
            "  448  564  737  728  370  207  875 2800  272  500  261  349  376  414\n",
            "  195  833 2843 1143 2229 1009  706 1129]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  564\n",
            "  448   98  737  272  875  370 2800  728  261  500  414  207  833  376\n",
            "  349 1143 2843 2229  195 1009 1163  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61 1612  359  503  786  158 1196  270 3686  448  569\n",
            "  737  564  728   98  370  207  875  272 2800  500  376  349  261  833\n",
            "  414 1143 2843 1009  706 2229 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  737\n",
            "  448   98  564  728  370 2800  207  500  875  261  272  376  414  833\n",
            "  349  195 2843 2229 1129 1009 1143 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61  359  503 1612  786 1196  158  270 3686  569  448\n",
            "  737  728   98  564 2800  370  500  875  261  272  207  376  414  349\n",
            "  833 2229 2843 1009  195 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  198  290  210 1307  435  471 1381\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  728  448\n",
            "   98  569  564  737  370 2800  272  500  875  207  414  833  261  349\n",
            "  376 1009 2229 2843 1143  195 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  290  198  210 1307  435 1381  471\n",
            "  231  355   27   61  359  786  503 1612  158 1196  270 3686  569  448\n",
            "   98  737  564  728  370  207 2800  875  272  500  349  414  376  261\n",
            " 1143  833  195 2229 1009 2843  706 1163]\n",
            "[  48   71  394    5 1039  307  300  198  210  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  569  737\n",
            "  564   98  728  448 2800  207  370  261  272  875  500  349  833  376\n",
            "  414 2843 2229 1009  195 1143 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  231 1381\n",
            "  471  355   27   61  503  359  786 1612  158 1196  270 3686   98  448\n",
            "  737  569  564  728  875  370 2800  207  272  500  261  376  833  414\n",
            "  349 2843 1009 2229  195 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  737  564\n",
            "   98  569  448  728  370  207  875 2800  272  261  349  500  376  414\n",
            "  833  195 2843 1009 1143 2229 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "  728  737  564   98  875  370  500 2800  261  272  207  833  349  376\n",
            "  414 2843  195 1143 1009 2229 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569  737\n",
            "   98  728  564  448  370 2800  875  272  207  261  500  414  833 2229\n",
            "  376  349 2843 1143 1009  195 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158 1196  270 3686  569  737\n",
            "  564   98  448  728  500 2800  875  370  272  207  261  376  414 2229\n",
            "  833  349 2843  195 1143 1009 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  728  564  448  737  370  500  261 2800  207  875  272  414  349 2229\n",
            "  376  833  195 2843 1143 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158  270 1196 3686  569  448\n",
            "   98  737  728  564 2800  370  272  875  261  207  414  349  376  500\n",
            " 2843  833 2229 1009  195 1129 1143  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  786  503 1612  158 1196  270 3686  569   98\n",
            "  737  448  564  370  728 2800  875  207  272  500  261  414  376  349\n",
            "  195 2229 2843 1143  833 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27  359   61 1612  503  786  158 1196  270 3686  448   98\n",
            "  728  564  569  737  500 2800  272  370  875  414  207  376  261  349\n",
            "  833  195 2229 2843 1143 1129 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  569  448\n",
            "  564   98  737  370 2800  728  875  261  500  414  272  207  349  376\n",
            "  195 2229 2843  833 1143 1129 1009 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27  359   61  503  786 1612  158 1196  270 3686  569   98\n",
            "  448  728  564  737  370  500 2800  272  207  875  261  414  833  349\n",
            "  376 2843 2229  195 1143 1009 3926 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27   61  359 1612  503  786  158  270 1196 3686  737  448\n",
            "  569  728  564 2800   98  207  272  875  261  370  500  414  349  376\n",
            "  833 2229 2843 1143  195 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61  359 1612  503  786  158 1196  270 3686  569   98\n",
            "  728  737  448  370 2800  564  261  500  875  207  414  272  376  833\n",
            "  349 2843 1009  195 2229  706 1143 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  786  503 1612  158 1196  270 3686  564   98\n",
            "  569  737  728  448  370 2800  875  207  272  500  414  261  376  833\n",
            "  349 2229 1009 2843  195 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  564\n",
            "  448   98  737  728 2800  370  207  875  272  500  261  376  349  414\n",
            "  833 1143 2843  195 2229 1009 3926 1163]\n",
            "[  48   71  394    5 1039  307  300  198  290  210 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "  564  737   98  728 2800  272  875  500  207  370  261  349  376  414\n",
            "  195 2229  833 1009 2843 1143 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  231 1381\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  448  728\n",
            "   98  569  737  370  564  207  875 2800  414  261  272  376  500  349\n",
            "  833  195 1143 2843 1009 2229 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "  737   98  564  728  875  272 2800  500  261  370  207  414  376  349\n",
            "  833 2229 2843  195 1143 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569   98\n",
            "  728  448  737  564  370  207  500 2800  272  875  261  414  376  349\n",
            "  833 2229 2843 1009 1143  195  706 1129]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27   61  359  786 1612  503  158 1196  270 3686  737  448\n",
            "  728   98  569 2800  875  272  564  500  207  370  376  261  349  833\n",
            " 1009  414 1143 2843  706 2229 1129  195]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  503  359 1612  786  158 1196  270 3686  569   98\n",
            "  737  448  728  564 2800  875  272  370  207  261  500  376  414  349\n",
            "  195  833 2229 2843 1143 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  231  471\n",
            " 1381  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  448  737  564  728  370 2800  272  261  875  500  207  414  376  349\n",
            " 2843  833 2229  195 1143 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  198  290  210 1307  435 1381  471\n",
            "  231  355   27   61  503  359  786 1612  158 1196  270 3686  564  448\n",
            "  737   98  569  370  728  207 2800  875  272  261  500  414  349  376\n",
            "  195  833 2229 2843 1009 1143 1129  389]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  231 1381\n",
            "  471  355   27   61  359  786  503 1612  158 1196  270 3686   98  448\n",
            "  728  564  569  737  370 2800  207  272  261  500  875  376  349  414\n",
            "  833  195 2843 1143 1009 2229 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  231  471\n",
            " 1381  355   27   61  359  786  503 1612  158 1196  270 3686  569   98\n",
            "  448  737  564  728  370  875 2800  500  207  272  261  414  349  376\n",
            "  833 2229 1143 2843 1009  195 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  737\n",
            "   98  448  564  728 2800  500  875  370  272  261  207  376  414  349\n",
            "  833  195 2229 2843 1143 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  448  564  728  737  370  207  500  875 2800  272  261  414  349  376\n",
            "  833 2843 2229 1009 1143 1129  195 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569  448\n",
            "  737  564   98  370  728  875 2800  500  207  261  272  414  376  349\n",
            " 2843  195 2229  833 1143 1129 1009  706]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569   98\n",
            "  448  564  728  737  370 2800  207  272  500  875  261  349  414  376\n",
            "  833 2229 2843  195 1143 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686   98  448\n",
            "  569  564  737  370  728  500  875  272 2800  207  261  414  376  349\n",
            "  833 2229 2843  195 1009 1143  706 1129]\n",
            "[  48   71  394    5 1039  307  300  290  198  210 1307  435 1381  471\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569  737\n",
            "  448  564   98  728  272 2800  261  207  370  875  414  349  500  376\n",
            "  833 2843 2229 1009 1143  195 1163  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  569  728\n",
            "  448  564   98  370  737  875  207  272 2800  349  500  261  414  376\n",
            "  833 1009 2229 1143 2843  195 1163  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686   98  569\n",
            "  737  448  564  728  875  370 2800  207  272  500  414  261  349  376\n",
            " 1143  833 2229 1009  195 2843 1129 1202]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686   98  448\n",
            "  737  569  564  728  370 2800  207  272  261  875  500  414  349  376\n",
            "  833 2843  195 2229 1009 1129 1143  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  569   98\n",
            "  737  448  728  564 2800  875  207  272  261  370  500  376  414  349\n",
            "  833 2229  195 1009 2843 1143 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27   61  359 1612  503  786 1196  158  270 3686  448   98\n",
            "  728  737  569  564  370  875  500 2800  207  272  261  376 2843  349\n",
            " 1143  833  414  195 1009 2229 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569   98\n",
            "  564  737  448  728  370 2800  207  272  875  500  261  833  376  414\n",
            "  349 2843  195 1143 1009 2229 1129  706]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "  564  737   98  728  207  272  370 2800  875  500  261  414  349  376\n",
            "  833 2843  195 2229 1143 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569   98\n",
            "  448  564  737  728  207  875 2800  500  370  261  272  414  833  349\n",
            "  376 2843  195 1143 1009 2229 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  564\n",
            "  737   98  448  728  370  875  207  272 2800  500  261  414  349  376\n",
            " 2229  833  195 1143 2843 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435  471 1381\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569  737\n",
            "   98  448  728  564  370  272 2800  875  414  500  207  261  376  833\n",
            "  349 2843 2229 1143  195 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686   98  448\n",
            "  569  370  737  728  564  875 2800  207  500  261  272  414  349  376\n",
            "  833 1143 2843  195 2229 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  198  210  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  737  564  448  370 2800  728  875  272  500  207  261  376  414  349\n",
            " 2229  195  833 2843 1143 3926 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  231 1381\n",
            "  471  355   27   61  359  503  786 1612  158 1196  270 3686  569   98\n",
            "  737  448  370  564  728 2800  875  261  500  272  207  376  349  833\n",
            "  414  195 2229 2843 1143 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  231 1381\n",
            "  471  355   27  359   61 1612  503  786  158  270 1196 3686   98  569\n",
            "  448  737  370  728  564  875 2800  272  500  207  414  261  349  376\n",
            "  833 2229  195 1143 2843 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307 1381  435  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  564\n",
            "   98  728  737 2800  448  875  370  272  207  261  500  414  376  349\n",
            " 2229  833  195 2843 1009 1143 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  231 1381\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686   98  737\n",
            "  448  728  564  569  875  370  272  500 2800  207  376  261  349  833\n",
            "  414 2843 2229  195 1009 1129  706 1143]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  503  359 1612  786  158 1196  270 3686  569  564\n",
            "  370   98  737  728  875  448  207 2800  272  261  349  500  414  376\n",
            " 1143  833 2229  195 2843 1009 1163  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  737  448  564  370  728 2800  500  875  414  272  207  261  376  349\n",
            "  833 2229 2843  195 1143 1129 1009  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  231\n",
            "  471  355   27   61  359  786  503 1612  158 1196  270 3686  569  448\n",
            "   98  737  728  564  875 2800  370  261  207  272  414  500  376  349\n",
            "  195  833 2843 1143 2229 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  231\n",
            "  471  355   27   61  359 1612  503  786  158 1196  270 3686  569  564\n",
            "  737  448  728   98  875  370 2800  272  261  207  414  500  376  833\n",
            " 2229  349 2843  195 1009 1143 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503  786 1612  158 1196  270 3686   98  569\n",
            "  728  448  564  737 2800  370  500  272  207  875  376  414  261  349\n",
            "  833 1143 2229 1009 2843  195  706 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307 1381  435  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  728  569\n",
            "   98  448  564  737  370 2800  272  207  500  875  261  414  376  833\n",
            " 2843  349  195 1143 2229 1129 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61  359  786  503 1612  158 1196  270 3686   98  737\n",
            "  448  564  569  370  728 2800  500  875  261  207  272  833  414  349\n",
            "  376 2229 2843  195 1009 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  737  569\n",
            "  728   98  564  448  261  370  272 2800  207  500  875  376  414  833\n",
            "  349 2843 2229 1009  195 1129 3926 1143]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158 1196  270 3686  569  448\n",
            "   98  737  370  564  728 2800  875  272  207  500  261  414  349  376\n",
            "  833 2229 2843  195 1143 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569   98\n",
            "  448  737  728  564  500  370  261 2800  875  272  207  414  376  349\n",
            "  833 2843 1009  195 2229 1143 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  786  503 1612 1196  158  270 3686  569   98\n",
            "  737  564  448  728  370  207  875 2800  500  414  272  376  261  349\n",
            "  195 2229 2843 1143  833 1009 1163 3926]\n",
            "[  48   71  394    5 1039  307  300  198  290  210 1307  435 1381  471\n",
            "  231  355   27  359   61  786  503 1612  158 1196  270 3686  564  728\n",
            "  370   98  448  569  737  875  500  261  272  207 2800  414  195  349\n",
            " 2843  833 2229  376 1009 1129 1143 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686   98  737\n",
            "  564  448  569  370  728 2800  875  500  272  207  261  414  349  376\n",
            " 2229  833 1143 2843  195 1129 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471  231\n",
            " 1381  355   27   61  503  359 1612  786  158 1196  270 3686  569  448\n",
            "   98  564  737  728  370 2800  875  207  261  500  272  349  414  376\n",
            "  833 2843 2229  195 1009 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "  564  728   98  737 2800  875  370  207  261  500  272  414  376  833\n",
            "  349 2843 2229  195 1129 1009 3926 1143]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359 1612  503  786  158 1196  270 3686   98  737\n",
            "  569  728  448  564  272 2800  207  500  370  875  261  414  376  349\n",
            "  833 2229  195 2843 1143 1009  706 1163]\n",
            "[  48   71  394    5 1039  307  300  198  290  210 1307  435  471 1381\n",
            "  231  355   61   27  359 1612  503  786  158 1196  270 3686   98  448\n",
            "  737  569  564  370  728  207  875 2800  272  500  414  261  376  349\n",
            " 2229 1009 2843  833 1143  195 1129  706]\n",
            "[  48   71  394    5 1039  307  300  198  210  290 1307  435 1381  471\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  569  564\n",
            "  737  448   98 2800  370  207  272  875  728  414  500  261  376  349\n",
            "  195  833 2843 2229 1009 1143 3926 1129]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  786  503 1612  158 1196  270 3686   98  569\n",
            "  737  564  370  448  875  728 2800  207  500  272  261  414  376  349\n",
            "  833 2229  195 2843 1143 1009 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27  359   61 1612  503  786  158 1196  270 3686  569   98\n",
            "  564  737  448  728  370  500  875  207 2800  261  272  414  376  349\n",
            " 2843  833  195 2229 1143 1009 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  569  728\n",
            "   98  737  564 2800  448  500  272  370  207  261  376  833  875  349\n",
            "  414 2229  195 2843 1009 1143 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  290  198  210 1307  435 1381  471\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  448  569\n",
            "  564  737  728   98  370  875 2800  207  261  272  500  414  349  376\n",
            " 2843  833 1009  195 1143 2229 3926 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  569   98\n",
            "  728  564  448  737  500  875 2800  370  207  261  272  376  349  414\n",
            "  195 2229  833 2843 1009 1143 1163 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  231\n",
            "  471  355   27   61  359  503  786 1612  158 1196  270 3686  569  564\n",
            "  737  448   98  728  370  207 2800  875  272  500  261  349  376  414\n",
            "  833  195 1143 2843 2229 1009  706 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  231\n",
            "  471  355   27   61  359 1612  503  786  158  270 1196 3686  448  569\n",
            "   98  737  564  728  875  370  500  207 2800  272  261  349  414  376\n",
            "  833  195 2843 2229 1143 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  198  210  290 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  737   98\n",
            "  569  448  564  370  875 2800  272  728  207  261  376  500  414  349\n",
            " 2843  833  195 1143 2229 1009 3926 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569   98\n",
            "  728  448  737  564  370  500 2800  875  207  272  376  414  261  349\n",
            " 2229  833  195 2843 1009 1143  706 1129]\n",
            "[  48   71  394    5 1039  307  300  290  198  210 1307  435  471 1381\n",
            "  231  355   27   61  359  503  786 1612  158 1196  270 3686  569   98\n",
            "  448  737  728  564 2800  875  207  272  370  414  261  349  500  376\n",
            "  833  195 1143 2843 1009 2229 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  471 1381\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  737  448\n",
            "   98  564  569  500  728  875  207 2800  272  370  261  376  833  349\n",
            " 2843  414 2229 1009 1143  195  706 1163]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27  359   61 1612  786  503  158 1196  270 3686  737   98\n",
            "  728  569  448  564  370  875  207 2800  272  500  414  261  833  376\n",
            "  349 1143  195 2229 1009 2843 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435 1381  471\n",
            "  231  355   27   61  503  359 1612  786  158 1196  270 3686  564  569\n",
            "  448  737   98  728 2800  207  370  500  272  875  261  414  833  376\n",
            "  349 2229  195 2843 1009 1143 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569   98\n",
            "  737  448  564  728 2800  875  207  370  272  261  376  414  500  349\n",
            "  833 2843 1143 1009  195 2229 1129 1163]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  737\n",
            "   98  448  370  728  564  207  272 2800  500  875  261  376  349  414\n",
            "  833 2229 2843  195 1143 1009 1129 3926]\n",
            "[  48   71  394    5 1039  307  300  210  198  290 1307  435  231 1381\n",
            "  471  355   27   61  359  503 1612  786  158 1196  270 3686  448  728\n",
            "  569  564   98  737  500  370  207 2800  875  272  261  414  376  349\n",
            "  833 1143 2229 1009  195 2843 1163 1129]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  231  355   27  359   61 1612  503  786  158 1196  270 3686   98  448\n",
            "  728  737  564  569  370  875  207 2800  261  272  500  414  376  833\n",
            "  349 2843 1143  195 2229 1009 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435  471 1381\n",
            "  355  231   27   61  359 1612  503  786 1196  158  270 3686  448   98\n",
            "  737  569  564  728 2800  370  272  500  207  875  376  833  261  414\n",
            " 2843 1009  349 2229 3926 1143 1129  195]\n",
            "[  48   71  394    5 1039  307  300  290  198  210 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158 1196  270 3686  569  448\n",
            "   98  564  728  737  370  272  500 2800  207  875  261  414  349  376\n",
            " 2229  833 2843  195 1129 1143 1009 1163]\n",
            "[  48   71  394    5 1039  307  300  290  210  198 1307  435 1381  471\n",
            "  231  355   27  359   61  503 1612  786  158 1196  270 3686  569  737\n",
            "  564  448   98  728  370  875  500  207  261  272 2800 2843  376  349\n",
            "  414  833 2229 1143 1009  195 1129  706]\n",
            "[  48   71  394    5 1039  307  300  210  290  198 1307  435 1381  471\n",
            "  231  355   27   61  359  503 1612  786  158  270 1196 3686  448  569\n",
            "  737  564   98  728  875  370 2800  207  261  272  500  349  376  833\n",
            "  414 1009 2229 1143  195 2843 3926  706]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNvz_G_5c1h"
      },
      "source": [
        "각 도서의 임베딩 벡터의 코사인 유사도(cosine similarity)를 기반으로, 특정 도서화 가장 연관 있는 도서를 추출하는 코드이다. 아래 코드는 책 id '673'인 도서를 기준으로 가장 연관 있는 도서 50권을 계산하는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7blBd9vfP1b",
        "outputId": "51322b5b-e860-4496-d655-77436ade39b8"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "book_vec = model.get_layer('book_vec').weights\n",
        "bias_vec = model.get_layer('book_bias_vec').weights\n",
        "\n",
        "book_vec = np.array(book_vec[0])\n",
        "bias_vec = np.array(bias_vec[0])\n",
        "\n",
        "# book_vec += bias_vec\n",
        "\n",
        "cosine_dic = {}\n",
        "v1 = book_vec[673].reshape(1, -1)\n",
        "for i in range(book_vec.shape[0]):\n",
        "  if i == 673:\n",
        "    continue\n",
        "  v2 = book_vec[i].reshape(1, -1)\n",
        "  cosine_dic[i] = float(cosine_similarity(v1, v2))\n",
        "\n",
        "cosine_dic = sorted(cosine_dic.items(), reverse=True, key = lambda x : x[1])\n",
        "for id, val in cosine_dic[0:50]:\n",
        "  print(id_to_book[id], val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Falling in Love (Commissario Brunetti, #24) 0.2828589379787445\n",
            "Giving Offense: Essays on Censorship 0.2824245095252991\n",
            "Roast Figs, Sugar Snow 0.26938021183013916\n",
            "The Shadow of Sirius 0.2576732337474823\n",
            "The Wind-Up Bird Chronicle 0.25722619891166687\n",
            "Learning from the Heart: Lessons on Living, Loving, and Listening 0.2530573010444641\n",
            "Station Eleven 0.2490752935409546\n",
            "The Lost Continent: Travels in Small Town America 0.24840736389160156\n",
            "Opening Belle 0.24748332798480988\n",
            "This Will Make You Smarter: New Scientific Concepts to Improve Your Thinking 0.24563416838645935\n",
            "Visitation Street 0.24195581674575806\n",
            "Lucy 0.24164488911628723\n",
            "About That Fling 0.24080528318881989\n",
            "Martin Dressler: The Tale of an American Dreamer 0.2363656759262085\n",
            "The First Four Books of Sampson Starkweather 0.22729706764221191\n",
            "On Dublin Street (On Dublin Street, #1) 0.22632721066474915\n",
            "Words Will Break Cement: The Passion of Pussy Riot 0.2252417355775833\n",
            "Girl in Translation 0.2216532826423645\n",
            "Breaking the Spell: Religion as a Natural Phenomenon 0.2208777219057083\n",
            "A Beautiful Day in the Neighborhood: The Poetry of Mister Rogers 0.22056743502616882\n",
            "Las tres EspaÃ±as del 36 0.2201635241508484\n",
            "Miracleman, Book Three: Olympus 0.22004449367523193\n",
            "Retromania: Pop Culture's Addiction to Its Own Past 0.21793793141841888\n",
            "The Girl in the Moss (Angie Pallorino, #3) 0.21710287034511566\n",
            "The Insanity Defense: The Complete Prose 0.2164417803287506\n",
            "Heart of Barkness (Chet and Bernie Mystery #9) 0.21506813168525696\n",
            "Peanut 0.2149617224931717\n",
            "The Black Echo / The Black Ice (Harry Bosch, #1-2) 0.21228183805942535\n",
            "Immediate Family 0.2120516449213028\n",
            "Practicing the Power of Now: Essential Teachings, Meditations, and Exercises from the Power of Now 0.2114141285419464\n",
            "The First Book of Swords (Books of Swords, #1) 0.21095220744609833\n",
            "La ciudad de los prodigios 0.21070003509521484\n",
            "The Saucepan Journey 0.20910115540027618\n",
            "FontBook 0.20900681614875793\n",
            "Viral Loop: From Facebook to Twitter, How Today's Smartest Businesses Grow Themselves 0.20885249972343445\n",
            "Prisoner of Tehran 0.20736248791217804\n",
            "Gold Coast 0.2062852829694748\n",
            "Little Nemo 0.20577864348888397\n",
            "Love Mode, Vol. 3 0.2056814730167389\n",
            "The Village of Stepanchikovo 0.20531366765499115\n",
            "Tatiana and Alexander (The Bronze Horseman, #2) 0.20523236691951752\n",
            "Not a Sound 0.20516397058963776\n",
            "Downtown Owl 0.20511788129806519\n",
            "Odd Hours (Odd Thomas, #4) 0.20450951159000397\n",
            "A Killer's Mind (Zoe Bentley Mystery, #1) 0.20434068143367767\n",
            "One More Thing: Stories and Other Stories 0.20405758917331696\n",
            "The Devil & Sherlock Holmes: Tales of Murder, Madness & Obsession 0.20267948508262634\n",
            "Chimera 0.20252510905265808\n",
            "Red 0.20179931819438934\n",
            "The Faiths of the Founding Fathers 0.20178571343421936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZMQpgE55-d9"
      },
      "source": [
        "평가데이터가 가장 많은 책 100권을 뽑아, csv 파일로 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tbMqGAjxnpy"
      },
      "source": [
        "id_to_book_1000 = num_book_sort[:100]\n",
        "\n",
        "id_1000 = [idx for idx, val in id_to_book_1000]\n",
        "book_1000 = [id_to_book[idx] for idx, val in id_to_book_1000]\n",
        "df = pd.DataFrame({'id': id_1000, 'book': book_1000})\n",
        "df.to_csv('book.csv', index=False) # 가장 평가 데이터가 많은 책 1000권을 따로 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOeVbNvW6Nke"
      },
      "source": [
        "위에서 추출한, 평가데이터가 가장 많은 책 100권을 기준으로, 가장 연관있는 도서 50권을 계산하는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VsvEcXE0Abm",
        "outputId": "606064b5-a403-450e-84d1-17b6be68ab87"
      },
      "source": [
        "cosine_dic_1000 = {}\n",
        "\n",
        "print(id_to_book[270])\n",
        "v1 = book_vec[270].reshape(1, -1)\n",
        "for key, val in id_to_book_1000:\n",
        "  if key == 270:\n",
        "    continue\n",
        "  v2 = book_vec[key].reshape(1, -1)\n",
        "  cosine_dic_1000[key] = float(cosine_similarity(v1, v2))\n",
        "\n",
        "cosine_dic_1000 = sorted(cosine_dic_1000.items(), reverse=True, key = lambda x : x[1])\n",
        "for id, val in cosine_dic_1000[0:50]:\n",
        "  print(id_to_book[id], val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1984\n",
            "Slaughterhouse-Five 0.4992491602897644\n",
            "Dress Your Family in Corduroy and Denim 0.4841062128543854\n",
            "Bel Canto 0.46693605184555054\n",
            "The Joy Luck Club 0.4445004463195801\n",
            "Romeo and Juliet 0.44134292006492615\n",
            "Atonement 0.44002407789230347\n",
            "The Kite Runner 0.4362126588821411\n",
            "The Grapes of Wrath 0.4337741732597351\n",
            "Harry Potter and the Half-Blood Prince (Harry Potter, #6) 0.43283364176750183\n",
            "The Glass Castle 0.4305427670478821\n",
            "Harry Potter and the Prisoner of Azkaban (Harry Potter, #3) 0.41996893286705017\n",
            "Naked 0.4173972010612488\n",
            "Macbeth 0.4053196609020233\n",
            "The Corrections 0.3979886472225189\n",
            "Fahrenheit 451 0.39794933795928955\n",
            "Harry Potter and the Sorcerer's Stone (Harry Potter, #1) 0.3976838290691376\n",
            "One Hundred Years of Solitude 0.3976699113845825\n",
            "Harry Potter and the Goblet of Fire (Harry Potter, #4) 0.38046836853027344\n",
            "Ender's Game (Ender's Saga, #1) 0.3740403652191162\n",
            "The Adventures of Huckleberry Finn 0.35969144105911255\n",
            "A Confederacy of Dunces 0.3435007929801941\n",
            "Gone with the Wind 0.3296218812465668\n",
            "The Alchemist 0.31676122546195984\n",
            "The Shadow of the Wind (The Cemetery of Forgotten Books, #1) 0.3128373920917511\n",
            "The Red Tent 0.3125814199447632\n",
            "To Kill a Mockingbird 0.3025818467140198\n",
            "Twilight (Twilight, #1) 0.3003796339035034\n",
            "Eclipse (Twilight, #3) 0.2914825677871704\n",
            "The Diary of a Young Girl 0.2777957320213318\n",
            "The Da Vinci Code (Robert Langdon, #2) 0.2668991982936859\n",
            "Harry Potter and the Deathly Hallows (Harry Potter, #7) 0.26292920112609863\n",
            "The Catcher in the Rye 0.2557567358016968\n",
            "The Poisonwood Bible 0.25391340255737305\n",
            "The Lovely Bones 0.22993113100528717\n",
            "The Help 0.22314760088920593\n",
            "The Great Gatsby 0.2003869116306305\n",
            "The Hunger Games (The Hunger Games, #1) 0.19709135591983795\n",
            "In Cold Blood 0.19611676037311554\n",
            "The Hobbit, or There and Back Again 0.1953386515378952\n",
            "The Handmaid's Tale (The Handmaid's Tale, #1) 0.19139032065868378\n",
            "Eat, Pray, Love 0.18236464262008667\n",
            "A Prayer for Owen Meany 0.1727515161037445\n",
            "The Omnivore's Dilemma: A Natural History of Four Meals 0.17167921364307404\n",
            "Ready Player One (Ready Player One, #1) 0.1451098769903183\n",
            "The Giver (The Giver, #1) 0.1364331990480423\n",
            "Of Mice and Men 0.13426177203655243\n",
            "All the Light We Cannot See 0.12685643136501312\n",
            "The Amazing Adventures of Kavalier & Clay 0.12020961195230484\n",
            "The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1) 0.10567593574523926\n",
            "Hamlet 0.0625375434756279\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}